{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "更换数据集MSRA和ERNIE-Gram或BERT等预训练模型。\n",
    "\n",
    "- 数据集：\n",
    "`train_ds, test_ds = load_dataset(\"msra_ner\", splits=[\"train\", \"test\"])`\n",
    "- 模型：\n",
    "\t将`from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification`换成相应的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleNLP语义预训练模型ERNIE完成快递单信息抽取\n",
    "\n",
    "\n",
    "**注意**\n",
    "\n",
    "本项目代码需要使用GPU环境来运行:\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/767f625548714f03b105b6ccb3aa78df9080e38d329e445380f505ddec6c7042\" width=\"40%\" height=\"40%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "命名实体识别是NLP中一项非常基础的任务，是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果，是NLP中的一个基础问题。在NER任务提供了两种解决方案，一类LSTM/GRU + CRF，通过RNN类的模型来抽取底层文本的信息，而CRF(条件随机场)模型来学习底层Token之间的联系；另外一类是通过预训练模型，例如ERNIE，BERT模型，直接来预测Token的标签信息。\n",
    "\n",
    "本项目将演示如何使用PaddleNLP语义预训练模型ERNIE完成从快递单中抽取姓名、电话、省、市、区、详细地址等内容，形成结构化信息。辅助物流行业从业者进行有效信息的提取，从而降低客户填单的成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在2017年之前，工业界和学术界对文本处理依赖于序列模型[Recurrent Neural Network (RNN)](https://baike.baidu.com/item/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/23199490?fromtitle=RNN&fromid=5707183&fr=aladdin).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\" width=\"40%\" height=\"30%\"> <br />\n",
    "</p><br><center>图1：RNN示意图</center></br>\n",
    "\n",
    "[基于BiGRU+CRF的快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)项目介绍了如何使用序列模型完成快递单信息抽取任务。\n",
    "<br>\n",
    "\n",
    "近年来随着深度学习的发展，模型参数的数量飞速增长。为了训练这些参数，需要更大的数据集来避免过拟合。然而，对于大部分NLP任务来说，构建大规模的标注数据集非常困难（成本过高），特别是对于句法和语义相关的任务。相比之下，大规模的未标注语料库的构建则相对容易。为了利用这些数据，我们可以先从其中学习到一个好的表示，再将这些表示应用到其他任务中。最近的研究表明，基于大规模未标注语料库的预训练模型（Pretrained Models, PTM) 在NLP任务上取得了很好的表现。\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的不断提高，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/327f44ff3ed24493adca5ddc4dc24bf61eebe67c84a6492f872406f464fde91e\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p><br><center>图2：预训练模型一览，图片来源于：https://github.com/thunlp/PLMpapers</center></br>\n",
    "                                                                                                                             \n",
    "本示例展示了以ERNIE([Enhanced Representation through Knowledge Integration](https://arxiv.org/pdf/1904.09223))为代表的预训练模型如何Finetune完成序列标注任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "开源不易，希望大家多多支持~ \n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_a\tlabel\r\n",
      "黑\u0002龙\u0002江\u0002省\u0002双\u0002鸭\u0002山\u0002市\u0002尖\u0002山\u0002区\u0002八\u0002马\u0002路\u0002与\u0002东\u0002平\u0002行\u0002路\u0002交\u0002叉\u0002口\u0002北\u00024\u00020\u0002米\u0002韦\u0002业\u0002涛\u00021\u00028\u00026\u00020\u00020\u00020\u00020\u00029\u00021\u00027\u00022\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\r\n",
      "广\u0002西\u0002壮\u0002族\u0002自\u0002治\u0002区\u0002桂\u0002林\u0002市\u0002雁\u0002山\u0002区\u0002雁\u0002山\u0002镇\u0002西\u0002龙\u0002村\u0002老\u0002年\u0002活\u0002动\u0002中\u0002心\u00021\u00027\u00026\u00021\u00020\u00023\u00024\u00028\u00028\u00028\u00028\u0002羊\u0002卓\u0002卫\tA1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "1\u00025\u00026\u00025\u00022\u00028\u00026\u00024\u00025\u00026\u00021\u0002河\u0002南\u0002省\u0002开\u0002封\u0002市\u0002顺\u0002河\u0002回\u0002族\u0002区\u0002顺\u0002河\u0002区\u0002公\u0002园\u0002路\u00023\u00022\u0002号\u0002赵\u0002本\u0002山\tT-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002P-B\u0002P-I\u0002P-I\r\n",
      "河\u0002北\u0002省\u0002唐\u0002山\u0002市\u0002玉\u0002田\u0002县\u0002无\u0002终\u0002大\u0002街\u00021\u00025\u00029\u0002号\u00021\u00028\u00026\u00021\u00024\u00022\u00025\u00023\u00020\u00025\u00028\u0002尚\u0002汉\u0002生\tA1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\r\n"
     ]
    }
   ],
   "source": [
    "# 下载并解压数据集\n",
    "from paddle.utils.download import get_path_from_url\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\n",
    "get_path_from_url(URL, \"./\")\n",
    "\n",
    "# 查看预测的数据\n",
    "!head -n 5 data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "# from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification\n",
    "from paddlenlp.transformers import BertTokenizer, BertForTokenClassification\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from utils import convert_example, evaluate, predict, load_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 加载自定义数据集\n",
    "\n",
    "推荐使用MapDataset()自定义数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def load_dataset(datafiles):\n",
    "#     def read(data_path):\n",
    "#         with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "#             next(fp)  # Skip header\n",
    "#             for line in fp.readlines():\n",
    "#                 words, labels = line.strip('\\n').split('\\t')\n",
    "#                 words = words.split('\\002')\n",
    "#                 labels = labels.split('\\002')\n",
    "#                 yield words, labels\n",
    "\n",
    "#     if isinstance(datafiles, str):\n",
    "#         return MapDataset(list(read(datafiles)))\n",
    "#     elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "#         return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# # Create dataset, tokenizer and dataloader.\n",
    "# train_ds, dev_ds, test_ds = load_dataset(datafiles=(\n",
    "#         './data/train.txt', './data/dev.txt', './data/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "# 由于MSRA_NER数据集没有dev dataset，我们这里重复加载test dataset作为dev_ds\r\n",
    "train_ds, dev_ds, test_ds = load_dataset(\r\n",
    "        'msra_ner', splits=('train', 'test', 'test'), lazy=False)\r\n",
    "\r\n",
    "# 注意删除 label_vocab = load_dict('./data/tag.dic')\r\n",
    "label_vocab = {label:label_id for label_id, label in enumerate(train_ds.label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n",
      "{'tokens': ['藏', '书', '本', '来', '就', '是', '所', '有', '传', '统', '收', '藏', '门', '类', '中', '的', '第', '一', '大', '户', '，', '只', '是', '我', '们', '结', '束', '温', '饱', '的', '时', '间', '太', '短', '而', '已', '。'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n",
      "{'tokens': ['因', '有', '关', '日', '寇', '在', '京', '掠', '夺', '文', '物', '详', '情', '，', '藏', '界', '较', '为', '重', '视', '，', '也', '是', '我', '们', '收', '藏', '北', '京', '史', '料', '中', '的', '要', '件', '之', '一', '。'], 'labels': [6, 6, 6, 4, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n",
      "{'tokens': ['我', '们', '藏', '有', '一', '册', '１', '９', '４', '５', '年', '６', '月', '油', '印', '的', '《', '北', '京', '文', '物', '保', '存', '保', '管', '状', '态', '之', '调', '查', '报', '告', '》', '，', '调', '查', '范', '围', '涉', '及', '故', '宫', '、', '历', '博', '、', '古', '研', '所', '、', '北', '大', '清', '华', '图', '书', '馆', '、', '北', '图', '、', '日', '伪', '资', '料', '库', '等', '二', '十', '几', '家', '，', '言', '及', '文', '物', '二', '十', '万', '件', '以', '上', '，', '洋', '洋', '三', '万', '余', '言', '，', '是', '珍', '贵', '的', '北', '京', '史', '料', '。'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 4, 5, 6, 2, 3, 3, 6, 4, 5, 5, 5, 5, 5, 5, 6, 4, 5, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 5, 6, 6, 6]}\n",
      "{'tokens': ['以', '家', '乡', '的', '历', '史', '文', '献', '、', '特', '定', '历', '史', '时', '期', '书', '刊', '、', '某', '一', '名', '家', '或', '名', '著', '的', '多', '种', '出', '版', '物', '为', '专', '题', '，', '注', '意', '精', '品', '、', '非', '卖', '品', '、', '纪', '念', '品', '，', '集', '成', '系', '列', '，', '那', '收', '藏', '的', '过', '程', '就', '已', '经', '够', '您', '玩', '味', '无', '穷', '了', '。'], 'labels': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_ds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签。\n",
    "\n",
    "之后，还需要对输入句子进行数据处理，如切词，映射词表id等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png\" hspace='10'/> <br />\n",
    "</p>\n",
    "<br><center>图3：ERNIE模型示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 07:48:36,757] [    INFO] - Found /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x7fb03d42cf50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_vocab = load_dict('./data/tag.dic')\n",
    "# tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer, label_vocab=label_vocab)\n",
    "\n",
    "train_ds.map(trans_func)\n",
    "dev_ds.map(trans_func)\n",
    "test_ds.map(trans_func)\n",
    "# print (train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据读入\n",
    "\n",
    "使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore_label = -1\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=36,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PaddleNLP一键加载预训练模型\n",
    "\n",
    "\n",
    "快递单信息抽取本质是一个序列标注任务，PaddleNLP对于各种预训练模型已经内置了对于下游任务文本分类Fine-tune网络。以下教程以ERNIE为预训练模型完成序列标注任务。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification()`一行代码即可加载预训练模型ERNIE用于序列标注任务的fine-tune网络。其在ERNIE模型后拼接上一个全连接网络进行分类。\n",
    "\n",
    "`paddlenlp.transformers.ErnieForTokenClassification.from_pretrained()`方法只需指定想要使用的模型名称和文本分类的类别数即可完成定义模型网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 07:48:36,798] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# Define the model netword and its loss\n",
    "# model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=len(label_vocab))\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_classes=len(label_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLP不仅支持ERNIE预训练模型，还支持BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。您可以使用PaddleNLP提供的模型，完成文本分类、序列标注、问答等任务。同时我们提供了众多预训练模型的参数权重供用户使用，其中包含了二十多种中文语言模型的预训练权重。中文的预训练模型有`bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, gpt2-base-cn, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small, chinese-xlnet-base, chinese-xlnet-mid, chinese-xlnet-large, unified_transformer-12L-cn, unified_transformer-12L-cn-luge`等。\n",
    "\n",
    "更多预训练模型参考：[PaddleNLP Transformer API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)。\n",
    "\n",
    "更多预训练模型fine-tune下游任务使用方法，请参考：[examples](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略，模型配置\n",
    "适用于ERNIE/BERT这类Transformer模型的迁移优化学习率策略为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p><br><center>图4：动态学习率示意图</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "loss_fn = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 - step:680 - loss: 0.018767\n",
      "epoch:0 - step:690 - loss: 0.044237\n",
      "epoch:0 - step:700 - loss: 0.023144\n",
      "epoch:0 - step:710 - loss: 0.064120\n",
      "epoch:0 - step:720 - loss: 0.021974\n",
      "epoch:0 - step:730 - loss: 0.055911\n",
      "epoch:0 - step:740 - loss: 0.051895\n",
      "epoch:0 - step:750 - loss: 0.018370\n",
      "epoch:0 - step:760 - loss: 0.020488\n",
      "epoch:0 - step:770 - loss: 0.026200\n",
      "epoch:0 - step:780 - loss: 0.058007\n",
      "epoch:0 - step:790 - loss: 0.040868\n",
      "epoch:0 - step:800 - loss: 0.020708\n",
      "epoch:0 - step:810 - loss: 0.014798\n",
      "epoch:0 - step:820 - loss: 0.034851\n",
      "epoch:0 - step:830 - loss: 0.016062\n",
      "epoch:0 - step:840 - loss: 0.069331\n",
      "epoch:0 - step:850 - loss: 0.026198\n",
      "epoch:0 - step:860 - loss: 0.031115\n",
      "epoch:0 - step:870 - loss: 0.030896\n",
      "epoch:0 - step:880 - loss: 0.016655\n",
      "epoch:0 - step:890 - loss: 0.013525\n",
      "epoch:0 - step:900 - loss: 0.100499\n",
      "epoch:0 - step:910 - loss: 0.030959\n",
      "epoch:0 - step:920 - loss: 0.018444\n",
      "epoch:0 - step:930 - loss: 0.009493\n",
      "epoch:0 - step:940 - loss: 0.029575\n",
      "epoch:0 - step:950 - loss: 0.007153\n",
      "epoch:0 - step:960 - loss: 0.009657\n",
      "epoch:0 - step:970 - loss: 0.135265\n",
      "epoch:0 - step:980 - loss: 0.081970\n",
      "epoch:0 - step:990 - loss: 0.009214\n",
      "epoch:0 - step:1000 - loss: 0.048931\n",
      "epoch:0 - step:1010 - loss: 0.028171\n",
      "epoch:0 - step:1020 - loss: 0.026437\n",
      "epoch:0 - step:1030 - loss: 0.001567\n",
      "epoch:0 - step:1040 - loss: 0.010433\n",
      "epoch:0 - step:1050 - loss: 0.044648\n",
      "epoch:0 - step:1060 - loss: 0.063121\n",
      "epoch:0 - step:1070 - loss: 0.037309\n",
      "epoch:0 - step:1080 - loss: 0.013192\n",
      "epoch:0 - step:1090 - loss: 0.008931\n",
      "epoch:0 - step:1100 - loss: 0.000832\n",
      "epoch:0 - step:1110 - loss: 0.017545\n",
      "epoch:0 - step:1120 - loss: 0.032134\n",
      "epoch:0 - step:1130 - loss: 0.027659\n",
      "epoch:0 - step:1140 - loss: 0.017181\n",
      "epoch:0 - step:1150 - loss: 0.046011\n",
      "epoch:0 - step:1160 - loss: 0.050967\n",
      "epoch:0 - step:1170 - loss: 0.008975\n",
      "epoch:0 - step:1180 - loss: 0.028484\n",
      "epoch:0 - step:1190 - loss: 0.015287\n",
      "epoch:0 - step:1200 - loss: 0.004782\n",
      "epoch:0 - step:1210 - loss: 0.020517\n",
      "epoch:0 - step:1220 - loss: 0.005530\n",
      "epoch:0 - step:1230 - loss: 0.019430\n",
      "epoch:0 - step:1240 - loss: 0.000649\n",
      "epoch:0 - step:1250 - loss: 0.036760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-12 07:53:31,545] [ WARNING] - Compatibility Warning: The params of ChunkEvaluator.compute has been modified. The old version is `inputs`, `lengths`, `predictions`, `labels` while the current version is `lengths`, `predictions`, `labels`.  Please update the usage.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: B-ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: I-ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: B-PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: I-PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: B-LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: I-LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval precision: 0.961798 - recall: 0.953648 - f1: 0.957706\n",
      "epoch:1 - step:1260 - loss: 0.027425\n",
      "epoch:1 - step:1270 - loss: 0.014600\n",
      "epoch:1 - step:1280 - loss: 0.026331\n",
      "epoch:1 - step:1290 - loss: 0.006822\n",
      "epoch:1 - step:1300 - loss: 0.041408\n",
      "epoch:1 - step:1310 - loss: 0.009605\n",
      "epoch:1 - step:1320 - loss: 0.012099\n",
      "epoch:1 - step:1330 - loss: 0.015018\n",
      "epoch:1 - step:1340 - loss: 0.028884\n",
      "epoch:1 - step:1350 - loss: 0.018188\n",
      "epoch:1 - step:1360 - loss: 0.024588\n",
      "epoch:1 - step:1370 - loss: 0.052471\n",
      "epoch:1 - step:1380 - loss: 0.024193\n",
      "epoch:1 - step:1390 - loss: 0.013425\n",
      "epoch:1 - step:1400 - loss: 0.032732\n",
      "epoch:1 - step:1410 - loss: 0.030388\n",
      "epoch:1 - step:1420 - loss: 0.012652\n",
      "epoch:1 - step:1430 - loss: 0.018046\n",
      "epoch:1 - step:1440 - loss: 0.028400\n",
      "epoch:1 - step:1450 - loss: 0.017144\n",
      "epoch:1 - step:1460 - loss: 0.049640\n",
      "epoch:1 - step:1470 - loss: 0.063475\n",
      "epoch:1 - step:1480 - loss: 0.016007\n",
      "epoch:1 - step:1490 - loss: 0.021898\n",
      "epoch:1 - step:1500 - loss: 0.010672\n",
      "epoch:1 - step:1510 - loss: 0.003101\n",
      "epoch:1 - step:1520 - loss: 0.009784\n",
      "epoch:1 - step:1530 - loss: 0.004434\n",
      "epoch:1 - step:1540 - loss: 0.000340\n",
      "epoch:1 - step:1550 - loss: 0.020749\n",
      "epoch:1 - step:1560 - loss: 0.000419\n",
      "epoch:1 - step:1570 - loss: 0.020851\n",
      "epoch:1 - step:1580 - loss: 0.000150\n",
      "epoch:1 - step:1590 - loss: 0.029065\n",
      "epoch:1 - step:1600 - loss: 0.001352\n",
      "epoch:1 - step:1610 - loss: 0.008915\n",
      "epoch:1 - step:1620 - loss: 0.018820\n",
      "epoch:1 - step:1630 - loss: 0.039106\n",
      "epoch:1 - step:1640 - loss: 0.032803\n",
      "epoch:1 - step:1650 - loss: 0.010880\n",
      "epoch:1 - step:1660 - loss: 0.018065\n",
      "epoch:1 - step:1670 - loss: 0.015785\n",
      "epoch:1 - step:1680 - loss: 0.026469\n",
      "epoch:1 - step:1690 - loss: 0.005955\n",
      "epoch:1 - step:1700 - loss: 0.002163\n",
      "epoch:1 - step:1710 - loss: 0.012101\n",
      "epoch:1 - step:1720 - loss: 0.033069\n",
      "epoch:1 - step:1730 - loss: 0.042885\n",
      "epoch:1 - step:1740 - loss: 0.001214\n",
      "epoch:1 - step:1750 - loss: 0.032935\n",
      "epoch:1 - step:1760 - loss: 0.038040\n",
      "epoch:1 - step:1770 - loss: 0.011688\n",
      "epoch:1 - step:1780 - loss: 0.019956\n",
      "epoch:1 - step:1790 - loss: 0.012318\n",
      "epoch:1 - step:1800 - loss: 0.044280\n",
      "epoch:1 - step:1810 - loss: 0.009346\n",
      "epoch:1 - step:1820 - loss: 0.011760\n",
      "epoch:1 - step:1830 - loss: 0.014209\n",
      "epoch:1 - step:1840 - loss: 0.007805\n",
      "epoch:1 - step:1850 - loss: 0.006540\n",
      "epoch:1 - step:1860 - loss: 0.002232\n",
      "epoch:1 - step:1870 - loss: 0.000271\n",
      "epoch:1 - step:1880 - loss: 0.020086\n",
      "epoch:1 - step:1890 - loss: 0.036124\n",
      "epoch:1 - step:1900 - loss: 0.006085\n",
      "epoch:1 - step:1910 - loss: 0.005691\n",
      "epoch:1 - step:1920 - loss: 0.027617\n",
      "epoch:1 - step:1930 - loss: 0.002727\n",
      "epoch:1 - step:1940 - loss: 0.026632\n",
      "epoch:1 - step:1950 - loss: 0.011498\n",
      "epoch:1 - step:1960 - loss: 0.031872\n",
      "epoch:1 - step:1970 - loss: 0.018258\n",
      "epoch:1 - step:1980 - loss: 0.034482\n",
      "epoch:1 - step:1990 - loss: 0.010010\n",
      "epoch:1 - step:2000 - loss: 0.003124\n",
      "epoch:1 - step:2010 - loss: 0.005633\n",
      "epoch:1 - step:2020 - loss: 0.005856\n",
      "epoch:1 - step:2030 - loss: 0.006351\n",
      "epoch:1 - step:2040 - loss: 0.018808\n",
      "epoch:1 - step:2050 - loss: 0.015598\n",
      "epoch:1 - step:2060 - loss: 0.000799\n",
      "epoch:1 - step:2070 - loss: 0.011556\n",
      "epoch:1 - step:2080 - loss: 0.008487\n",
      "epoch:1 - step:2090 - loss: 0.020110\n",
      "epoch:1 - step:2100 - loss: 0.000632\n",
      "epoch:1 - step:2110 - loss: 0.032840\n",
      "epoch:1 - step:2120 - loss: 0.031697\n",
      "epoch:1 - step:2130 - loss: 0.006521\n",
      "epoch:1 - step:2140 - loss: 0.003083\n",
      "epoch:1 - step:2150 - loss: 0.039310\n",
      "epoch:1 - step:2160 - loss: 0.013813\n",
      "epoch:1 - step:2170 - loss: 0.002367\n",
      "epoch:1 - step:2180 - loss: 0.002811\n",
      "epoch:1 - step:2190 - loss: 0.015294\n",
      "epoch:1 - step:2200 - loss: 0.005509\n",
      "epoch:1 - step:2210 - loss: 0.001310\n",
      "epoch:1 - step:2220 - loss: 0.042471\n",
      "epoch:1 - step:2230 - loss: 0.031734\n",
      "epoch:1 - step:2240 - loss: 0.007548\n",
      "epoch:1 - step:2250 - loss: 0.033739\n",
      "epoch:1 - step:2260 - loss: 0.018673\n",
      "epoch:1 - step:2270 - loss: 0.034081\n",
      "epoch:1 - step:2280 - loss: 0.000281\n",
      "epoch:1 - step:2290 - loss: 0.003050\n",
      "epoch:1 - step:2300 - loss: 0.022253\n",
      "epoch:1 - step:2310 - loss: 0.039559\n",
      "epoch:1 - step:2320 - loss: 0.014418\n",
      "epoch:1 - step:2330 - loss: 0.021497\n",
      "epoch:1 - step:2340 - loss: 0.002875\n",
      "epoch:1 - step:2350 - loss: 0.000659\n",
      "epoch:1 - step:2360 - loss: 0.008613\n",
      "epoch:1 - step:2370 - loss: 0.038260\n",
      "epoch:1 - step:2380 - loss: 0.014786\n",
      "epoch:1 - step:2390 - loss: 0.013577\n",
      "epoch:1 - step:2400 - loss: 0.007230\n",
      "epoch:1 - step:2410 - loss: 0.018983\n",
      "epoch:1 - step:2420 - loss: 0.006455\n",
      "epoch:1 - step:2430 - loss: 0.016170\n",
      "epoch:1 - step:2440 - loss: 0.004138\n",
      "epoch:1 - step:2450 - loss: 0.001731\n",
      "epoch:1 - step:2460 - loss: 0.009468\n",
      "epoch:1 - step:2470 - loss: 0.000958\n",
      "epoch:1 - step:2480 - loss: 0.003944\n",
      "epoch:1 - step:2490 - loss: 0.000199\n",
      "epoch:1 - step:2500 - loss: 0.016500\n",
      "eval precision: 0.968772 - recall: 0.955542 - f1: 0.962112\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(2):\n",
    "    for idx, (input_ids, token_type_ids, length, labels) in enumerate(train_loader):\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = paddle.mean(loss_fn(logits, labels))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            print(\"epoch:%d - step:%d - loss: %f\" % (epoch, step, loss))\n",
    "    evaluate(model, metric, dev_loader)\n",
    "\n",
    "    paddle.save(model.state_dict(),\n",
    "                './bert_result/model_%d.pdparams' % step)\n",
    "# model.save_pretrained('./checkpoint')\n",
    "# tokenizer.save_pretrained('./checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型预测\n",
    "\n",
    "训练保存好的模型，即可用于预测。如以下示例代码自定义预测数据，调用`predict()`函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results have been saved in the file: ernie_results.txt, some examples are shown below: \n",
      "('中共中央', 'ORG')('致', 'O')('中国致公党十一大', 'ORG')('的', 'O')('贺', 'O')('词', 'O')('各', 'O')('位', 'O')('代', 'O')('表', 'O')('、', 'O')('各', 'O')('位', 'O')('同', 'O')('志', 'O')('：', 'O')('在', 'O')('中国致公党第十一次全国代表大会', 'ORG')('隆', 'O')('重', 'O')('召', 'O')('开', 'O')('之', 'O')('际', 'O')('，', 'O')('中国共产党中央委员会', 'ORG')('谨', 'O')('向', 'O')('大', 'O')('会', 'O')('表', 'O')('示', 'O')('热', 'O')('烈', 'O')('的', 'O')('祝', 'O')('贺', 'O')('，', 'O')('向', 'O')('致', 'O')('公党', 'O')('的', 'O')('同', 'O')('志', 'O')('们', 'O')('致', 'O')('以', 'O')('亲', 'O')('切', 'O')('的', 'O')('问', 'O')('候', 'O')('！', 'O')\n",
      "('在', 'O')('过', 'O')('去', 'O')('的', 'O')('五', 'O')('年', 'O')('中', 'O')('，', 'O')('致公党', 'ORG')('在', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('遵', 'O')('循', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('初', 'O')('级', 'O')('阶', 'O')('段', 'O')('的', 'O')('基', 'O')('本', 'O')('路', 'O')('线', 'O')('，', 'O')('努', 'O')('力', 'O')('实', 'O')('践', 'O')('致公党十大', 'ORG')('提', 'O')('出', 'O')('的', 'O')('发', 'O')('挥', 'O')('参', 'O')('政', 'O')('党', 'O')('职', 'O')('能', 'O')('、', 'O')('加', 'O')('强', 'O')('自', 'O')('身', 'O')('建', 'O')('设', 'O')('的', 'O')('基', 'O')('本', 'O')('任', 'O')('务', 'O')('。', 'O')\n",
      "('高', 'O')('举', 'O')('爱', 'O')('国', 'O')('主', 'O')('义', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('两', 'O')('面', 'O')('旗', 'O')('帜', 'O')('，', 'O')('团', 'O')('结', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('以', 'O')('及', 'O')('所', 'O')('联', 'O')('系', 'O')('的', 'O')('归', 'O')('侨', 'O')('、', 'O')('侨', 'O')('眷', 'O')('，', 'O')('发', 'O')('扬', 'O')('爱', 'O')('国', 'O')('革', 'O')('命', 'O')('的', 'O')('光', 'O')('荣', 'O')('传', 'O')('统', 'O')('，', 'O')('为', 'O')('统', 'O')('一', 'O')('祖', 'O')('国', 'O')('、', 'O')('振', 'O')('兴', 'O')('中华', 'LOC')('而', 'O')('努', 'O')('力', 'O')('奋', 'O')('斗', 'O')('；', 'O')('紧', 'O')('紧', 'O')('围', 'O')('绕', 'O')('国', 'O')('家', 'O')('的', 'O')('中', 'O')('心', 'O')('工', 'O')('作', 'O')('，', 'O')('联', 'O')('系', 'O')('改', 'O')('革', 'O')('和', 'O')('建', 'O')('设', 'O')('中', 'O')('的', 'O')('重', 'O')('大', 'O')('问', 'O')('题', 'O')('以', 'O')('及', 'O')('人', 'O')('民', 'O')('群', 'O')('众', 'O')('普', 'O')('遍', 'O')('关', 'O')('心', 'O')('的', 'O')('社', 'O')('会', 'O')('问', 'O')('题', 'O')('，', 'O')('深', 'O')('入', 'O')('开', 'O')('展', 'O')('调', 'O')('查', 'O')('研', 'O')('究', 'O')('，', 'O')('就', 'O')('经', 'O')('济', 'O')('建', 'O')('设', 'O')('、', 'O')('侨', 'O')('务', 'O')('政', 'O')('策', 'O')('、', 'O')('文', 'O')('教', 'O')('卫', 'O')('生', 'O')\n",
      "('在', 'O')('此', 'O')('，', 'O')('中共中央', 'ORG')('谨', 'O')('向', 'O')('致公党中央', 'O')('以', 'O')('及', 'O')('全', 'O')('体', 'O')('成', 'O')('员', 'O')('致', 'O')('以', 'O')('崇', 'O')('高', 'O')('的', 'O')('敬', 'O')('意', 'O')('！', 'O')\n",
      "('不', 'O')('久', 'O')('前', 'O')('，', 'O')('中国共产党', 'ORG')('召', 'O')('开', 'O')('了', 'O')('举', 'O')('世', 'O')('瞩', 'O')('目', 'O')('的', 'O')('第十五次全国代表大会', 'ORG')('。', 'O')\n",
      "('这', 'O')('次', 'O')('代', 'O')('表', 'O')('大', 'O')('会', 'O')('是', 'O')('在', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('发', 'O')('展', 'O')('的', 'O')('关', 'O')('键', 'O')('时', 'O')('刻', 'O')('召', 'O')('开', 'O')('的', 'O')('历', 'O')('史', 'O')('性', 'O')('会', 'O')('议', 'O')('。', 'O')\n",
      "('大', 'O')('会', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('回', 'O')('顾', 'O')('一', 'O')('个', 'O')('世', 'O')('纪', 'O')('以', 'O')('来', 'O')('中国', 'LOC')('人', 'O')('民', 'O')('的', 'O')('奋', 'O')('斗', 'O')('历', 'O')('史', 'O')('，', 'O')('展', 'O')('望', 'O')('下', 'O')('个', 'O')('世', 'O')('纪', 'O')('５', 'O')('０', 'O')('年', 'O')('的', 'O')('发', 'O')('展', 'O')('前', 'O')('景', 'O')('，', 'O')('认', 'O')('真', 'O')('总', 'O')('结', 'O')('了', 'O')('中共', 'ORG')('十', 'O')('一', 'O')('届', 'O')('三', 'O')('中', 'O')('全', 'O')('会', 'O')('以', 'O')('来', 'O')('特', 'O')('别', 'O')('是', 'O')('十四大', 'ORG')('以', 'O')('来', 'O')('的', 'O')('实', 'O')('践', 'O')('经', 'O')('验', 'O')('，', 'O')('对', 'O')('中国', 'LOC')('改', 'O')('革', 'O')('开', 'O')('放', 'O')('和', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('现', 'O')('代', 'O')('化', 'O')('建', 'O')('设', 'O')('跨', 'O')('世', 'O')('纪', 'O')('的', 'O')('发', 'O')('展', 'O')('作', 'O')('出', 'O')('了', 'O')('全', 'O')('面', 'O')('部', 'O')('署', 'O')('。', 'O')\n",
      "('这', 'O')('次', 'O')('大', 'O')('会', 'O')('对', 'O')('于', 'O')('动', 'O')('员', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('，', 'O')('解', 'O')('放', 'O')('思', 'O')('想', 'O')('，', 'O')('实', 'O')('事', 'O')('求', 'O')('是', 'O')('，', 'O')('抓', 'O')('住', 'O')('有', 'O')('利', 'O')('时', 'O')('机', 'O')('，', 'O')('继', 'O')('续', 'O')('开', 'O')('拓', 'O')('前', 'O')('进', 'O')('，', 'O')('把', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('伟', 'O')('大', 'O')('事', 'O')('业', 'O')('全', 'O')('面', 'O')('推', 'O')('向', 'O')('２', 'O')('１', 'O')('世', 'O')('纪', 'O')('，', 'O')('具', 'O')('有', 'O')('极', 'O')('其', 'O')('重', 'O')('大', 'O')('和', 'O')('深', 'O')('远', 'O')('的', 'O')('意', 'O')('义', 'O')('。', 'O')\n",
      "('当', 'O')('前', 'O')('，', 'O')('在', 'O')('中共十五大', 'ORG')('精', 'O')('神', 'O')('的', 'O')('指', 'O')('引', 'O')('下', 'O')('，', 'O')('在', 'O')('以', 'O')('江泽民', 'PER')('同', 'O')('志', 'O')('为', 'O')('核', 'O')('心', 'O')('的', 'O')('中共中央', 'ORG')('领', 'O')('导', 'O')('下', 'O')('，', 'O')('全', 'O')('党', 'O')('和', 'O')('全', 'O')('国', 'O')('各', 'O')('族', 'O')('人', 'O')('民', 'O')('正', 'O')('高', 'O')('举', 'O')('邓小平', 'PER')('理', 'O')('论', 'O')('伟', 'O')('大', 'O')('旗', 'O')('帜', 'O')('，', 'O')('同', 'O')('心', 'O')('同', 'O')('德', 'O')('，', 'O')('团', 'O')('结', 'O')('奋', 'O')('斗', 'O')('，', 'O')('沿', 'O')('着', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('的', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('道', 'O')('路', 'O')('阔', 'O')('步', 'O')('前', 'O')('进', 'O')('。', 'O')\n",
      "('实', 'O')('现', 'O')('建', 'O')('设', 'O')('有', 'O')('中国', 'LOC')('特', 'O')('色', 'O')('社', 'O')('会', 'O')('主', 'O')('义', 'O')('的', 'O')('宏', 'O')('伟', 'O')('目', 'O')('标', 'O')('，', 'O')('是', 'O')('中国共产党', 'ORG')('和', 'O')('作', 'O')('为', 'O')('参', 'O')('政', 'O')('党', 'O')('的', 'O')('各', 'O')('民', 'O')('主', 'O')('党', 'O')('派', 'O')('共', 'O')('同', 'O')('肩', 'O')('负', 'O')('的', 'O')('历', 'O')('史', 'O')('使', 'O')('命', 'O')('。', 'O')\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, test_loader, test_ds, label_vocab)\n",
    "file_path = \"ernie_results.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write(\"\\n\".join(preds))\n",
    "# Print some examples\n",
    "print(\n",
    "    \"The results have been saved in the file: %s, some examples are shown below: \"\n",
    "    % file_path)\n",
    "print(\"\\n\".join(preds[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进一步使用CRF\n",
    "\n",
    "PaddleNLP提供了CRF Layer，它能够学习label之间的关系，能够帮助模型更好地学习、预测序列标注任务。\n",
    "\n",
    "我们在PaddleNLP仓库中提供了[示例](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/examples/information_extraction/waybill_ie/run_ernie_crf.py)，您可以参照示例代码使用Ernie-CRF结构完成快递单信息抽取任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入课程QQ交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
