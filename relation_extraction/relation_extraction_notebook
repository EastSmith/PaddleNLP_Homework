{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业\n",
    "\n",
    "- 补全程序中的代码，理解其含义，并跑通整个项目；\n",
    "- 报名参加[千言数据集：信息抽取比赛](https://aistudio.baidu.com/aistudio/competition/detail/46)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "比赛提交结果：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e3175dcddd714469a0405bc6165a37a2ed4f688780eb479dbd64f9da6bd09963)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/79611c8946b14986b5714887da317f0e59be995f10df42dd8323256540f6fca2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于预训练模型完成实体关系抽取\n",
    "\n",
    "信息抽取旨在从非结构化自然语言文本中提取结构化知识，如实体、关系、事件等。对于给定的自然语言句子，根据预先定义的schema集合，抽取出所有满足schema约束的SPO三元组。\n",
    "\n",
    "例如，「妻子」关系的schema定义为：      \n",
    "{      \n",
    "    S_TYPE: 人物,        \n",
    "    P: 妻子,      \n",
    "    O_TYPE: {      \n",
    "        @value: 人物       \n",
    "    }       \n",
    "}        \n",
    "\n",
    "该示例展示了如何使用PaddleNLP快速完成实体关系抽取，参与[千言信息抽取-关系抽取比赛](https://aistudio.baidu.com/aistudio/competition/detail/46)打榜。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 安装paddlenlp最新版本\n",
    "!pip install --upgrade paddlenlp\n",
    "\n",
    "%cd relation_extraction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 关系抽取介绍\n",
    "\n",
    "针对 DuIE2.0 任务中多条、交叠SPO这一抽取目标，比赛对标准的 'BIO' 标注进行了扩展。\n",
    "对于每个 token，根据其在实体span中的位置（包括B、I、O三种），我们为其打上三类标签，并且根据其所参与构建的predicate种类，将 B 标签进一步区分。给定 schema 集合，对于 N 种不同 predicate，以及头实体/尾实体两种情况，我们设计对应的共 2*N 种 B 标签，再合并 I 和 O 标签，故每个 token 一共有 (2*N+2) 个标签，如下图所示。\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/f984664777b241a9b43ef843c9b752f33906c8916bc146a69f7270b5858bee63\" width=\"500\" height=\"400\" alt=\"标注策略\" align=center />\n",
    "</div>\n",
    "\n",
    "### 评价方法\n",
    "\n",
    "对测试集上参评系统输出的SPO结果和人工标注的SPO结果进行精准匹配，采用F1值作为评价指标。注意，对于复杂O值类型的SPO，必须所有槽位都精确匹配才认为该SPO抽取正确。针对部分文本中存在实体别名的问题，使用百度知识图谱的别名词典来辅助评测。F1值的计算方式如下：\n",
    "\n",
    "F1 = (2 * P * R) / (P + R)，其中\n",
    "\n",
    "- P = 测试集所有句子中预测正确的SPO个数 / 测试集所有句子中预测出的SPO个数\n",
    "- R = 测试集所有句子中预测正确的SPO个数 / 测试集所有句子中人工标注的SPO个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step1：构建模型\n",
    "\n",
    "该任务可以看作一个序列标注任务，所以基线模型采用的是ERNIE序列标注模型。\n",
    "\n",
    "**PaddleNLP提供了ERNIE预训练模型常用序列标注模型，可以通过指定模型名字完成一键加载。PaddleNLP为了方便用户处理数据，内置了对于各个预训练模型对应的Tokenizer，可以完成文本token化，转token ID，文本长度截断等操作。**\n",
    "\n",
    "文本数据处理直接调用tokenizer即可输出模型所需输入数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-14 21:33:52,815] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-14 21:34:03,921] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer\n",
    "\n",
    "label_map_path = os.path.join('data', \"predicate2id.json\")\n",
    "\n",
    "if not (os.path.exists(label_map_path) and os.path.isfile(label_map_path)):\n",
    "    sys.exit(\"{} dose not exists or is not a file.\".format(label_map_path))\n",
    "with open(label_map_path, 'r', encoding='utf8') as fp:\n",
    "    label_map = json.load(fp)\n",
    "    \n",
    "num_classes = (len(label_map.keys()) - 2) * 2 + 2\n",
    "\n",
    "# # 补齐代码，理解TokenClassification接口含义，理解关系抽取标注体系和类别数由来\n",
    "# model = ErnieForTokenClassification.from_pretrained(\"ernie-1.0\", num_classes=(len(label_map) - 2) * 2 + 2)\n",
    "# tokenizer = ErnieTokenizer.from_pretrained(\"ernie-1.0\")\n",
    "\n",
    "from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-wwm-ext-large\",\n",
    "    num_classes=(len(label_map) - 2) * 2 + 2)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")\n",
    "\n",
    "inputs = tokenizer(text=\"请输入测试样例\", max_seq_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step2：加载并处理数据\n",
    "\n",
    "\n",
    "从比赛官网下载数据集，解压存放于data/目录下并重命名为train_data.json, dev_data.json, test_data.json.\n",
    "\n",
    "我们可以加载自定义数据集。通过继承[`paddle.io.Dataset`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/Dataset_cn.html#dataset)，自定义实现`__getitem__` 和 `__len__`两个方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union, Dict\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "from tqdm import tqdm\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "from data_loader import parse_label, DataCollator, convert_example_to_feature\n",
    "from extract_chinese_and_punct import ChineseAndPunctuationExtractor\n",
    "\n",
    "\n",
    "class DuIEDataset(paddle.io.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of DuIE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_ids: List[Union[List[int], np.ndarray]],\n",
    "            seq_lens: List[Union[List[int], np.ndarray]],\n",
    "            tok_to_orig_start_index: List[Union[List[int], np.ndarray]],\n",
    "            tok_to_orig_end_index: List[Union[List[int], np.ndarray]],\n",
    "            labels: List[Union[List[int], np.ndarray, List[str], List[Dict]]]):\n",
    "        super(DuIEDataset, self).__init__()\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.seq_lens = seq_lens\n",
    "        self.tok_to_orig_start_index = tok_to_orig_start_index\n",
    "        self.tok_to_orig_end_index = tok_to_orig_end_index\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        if isinstance(self.input_ids, np.ndarray):\n",
    "            return self.input_ids.shape[0]\n",
    "        else:\n",
    "            return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {\n",
    "            \"input_ids\": np.array(self.input_ids[item]),\n",
    "            \"seq_lens\": np.array(self.seq_lens[item]),\n",
    "            \"tok_to_orig_start_index\":\n",
    "            np.array(self.tok_to_orig_start_index[item]),\n",
    "            \"tok_to_orig_end_index\": np.array(self.tok_to_orig_end_index[item]),\n",
    "            # If model inputs is generated in `collate_fn`, delete the data type casting.\n",
    "            \"labels\": np.array(\n",
    "                self.labels[item], dtype=np.float32),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls,\n",
    "                  file_path: Union[str, os.PathLike],\n",
    "                  tokenizer: ErnieTokenizer,\n",
    "                  max_length: Optional[int]=512,\n",
    "                  pad_to_max_length: Optional[bool]=None):\n",
    "        assert os.path.exists(file_path) and os.path.isfile(\n",
    "            file_path), f\"{file_path} dose not exists or is not a file.\"\n",
    "        label_map_path = os.path.join(\n",
    "            os.path.dirname(file_path), \"predicate2id.json\")\n",
    "        assert os.path.exists(label_map_path) and os.path.isfile(\n",
    "            label_map_path\n",
    "        ), f\"{label_map_path} dose not exists or is not a file.\"\n",
    "        with open(label_map_path, 'r', encoding='utf8') as fp:\n",
    "            label_map = json.load(fp)\n",
    "        chineseandpunctuationextractor = ChineseAndPunctuationExtractor()\n",
    "\n",
    "        input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = (\n",
    "            [] for _ in range(5))\n",
    "        dataset_scale = sum(1 for line in open(file_path, 'r'))\n",
    "        logger.info(\"Preprocessing data, loaded from %s\" % file_path)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            lines = fp.readlines()\n",
    "            for line in tqdm(lines):\n",
    "                example = json.loads(line)\n",
    "                input_feature = convert_example_to_feature(\n",
    "                    example, tokenizer, chineseandpunctuationextractor,\n",
    "                    label_map, max_length, pad_to_max_length)\n",
    "                input_ids.append(input_feature.input_ids)\n",
    "                seq_lens.append(input_feature.seq_len)\n",
    "                tok_to_orig_start_index.append(\n",
    "                    input_feature.tok_to_orig_start_index)\n",
    "                tok_to_orig_end_index.append(\n",
    "                    input_feature.tok_to_orig_end_index)\n",
    "                labels.append(input_feature.labels)\n",
    "\n",
    "        return cls(input_ids, seq_lens, tok_to_orig_start_index,\n",
    "                   tok_to_orig_end_index, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-14 21:34:04,267] [    INFO] - Preprocessing data, loaded from data/train.json\n",
      "100%|██████████| 171293/171293 [05:21<00:00, 532.79it/s]\n",
      "[2021-06-14 21:39:26,184] [    INFO] - Preprocessing data, loaded from data/dev.json\n",
      "100%|██████████| 20674/20674 [00:38<00:00, 530.95it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "batch_size = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "train_file_path = os.path.join(data_path, 'train.json') ##train_data.json\n",
    "train_dataset = DuIEDataset.from_file(\n",
    "    train_file_path, tokenizer, max_seq_length, True)\n",
    "train_batch_sampler = paddle.io.BatchSampler(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "collator = DataCollator()\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    collate_fn=collator)\n",
    "\n",
    "eval_file_path = os.path.join(data_path, 'dev.json') ## dev_data.json\n",
    "test_dataset = DuIEDataset.from_file(\n",
    "    eval_file_path, tokenizer, max_seq_length, True)\n",
    "test_batch_sampler = paddle.io.BatchSampler(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step3：定义损失函数和优化器，开始训练\n",
    "\n",
    "我们选择均方误差作为损失函数，使用[`paddle.optimizer.AdamW`](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/adamw/AdamW_cn.html#adamw)作为优化器。\n",
    "\n",
    "\n",
    "\n",
    "在训练过程中，模型保存在当前目录checkpoints文件夹下。同时在训练的同时使用官方评测脚本进行评估，输出P/R/F1指标。\n",
    "在验证集上F1可以达到69.42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "\n",
    "class BCELossForDuIE(nn.Layer):\n",
    "    def __init__(self, ):\n",
    "        super(BCELossForDuIE, self).__init__()\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels, mask):\n",
    "        loss = self.criterion(logits, labels)\n",
    "        mask = paddle.cast(mask, 'float32')\n",
    "        loss = loss * mask.unsqueeze(-1)\n",
    "        loss = paddle.sum(loss.mean(axis=2), axis=1) / paddle.sum(mask, axis=1)\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import write_prediction_results, get_precision_recall_f1, decoding\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, data_loader, file_path, mode):\n",
    "    \"\"\"\n",
    "    mode eval:\n",
    "    eval on development set and compute P/R/F1, called between training.\n",
    "    mode predict:\n",
    "    eval on development / test set, then write predictions to \\\n",
    "        predict_test.json and predict_test.json.zip \\\n",
    "        under /home/aistudio/relation_extraction/data dir for later submission or evaluation.\n",
    "    \"\"\"\n",
    "    example_all = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            example_all.append(json.loads(line))\n",
    "    id2spo_path = os.path.join(os.path.dirname(file_path), \"id2spo.json\")\n",
    "    with open(id2spo_path, 'r', encoding='utf8') as fp:\n",
    "        id2spo = json.load(fp)\n",
    "\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "    eval_steps = 0\n",
    "    formatted_outputs = []\n",
    "    current_idx = 0\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        eval_steps += 1\n",
    "        input_ids, seq_len, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch\n",
    "        logits = model(input_ids=input_ids)\n",
    "        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and((input_ids != 2))\n",
    "        loss = criterion(logits, labels, mask)\n",
    "        loss_all += loss.numpy().item()\n",
    "        probs = F.sigmoid(logits)\n",
    "        logits_batch = probs.numpy()\n",
    "        seq_len_batch = seq_len.numpy()\n",
    "        tok_to_orig_start_index_batch = tok_to_orig_start_index.numpy()\n",
    "        tok_to_orig_end_index_batch = tok_to_orig_end_index.numpy()\n",
    "        formatted_outputs.extend(decoding(example_all[current_idx: current_idx+len(logits)],\n",
    "                                          id2spo,\n",
    "                                          logits_batch,\n",
    "                                          seq_len_batch,\n",
    "                                          tok_to_orig_start_index_batch,\n",
    "                                          tok_to_orig_end_index_batch))\n",
    "        current_idx = current_idx+len(logits)\n",
    "    loss_avg = loss_all / eval_steps\n",
    "    print(\"eval loss: %f\" % (loss_avg))\n",
    "\n",
    "    if mode == \"predict\":\n",
    "        predict_file_path = os.path.join(\"/home/aistudio/relation_extraction/data\", 'predictions.json')\n",
    "    else:\n",
    "        predict_file_path = os.path.join(\"/home/aistudio/relation_extraction/data\", 'predict_eval.json')\n",
    "\n",
    "    predict_zipfile_path = write_prediction_results(formatted_outputs,\n",
    "                                                    predict_file_path)\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        precision, recall, f1 = get_precision_recall_f1(file_path,\n",
    "                                                        predict_zipfile_path)\n",
    "        os.system('rm {} {}'.format(predict_file_path, predict_zipfile_path))\n",
    "        return precision, recall, f1\n",
    "    elif mode != \"predict\":\n",
    "        raise Exception(\"wrong mode for eval func\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 5\n",
    "warmup_ratio = 0.06\n",
    "\n",
    "criterion = BCELossForDuIE()\n",
    "# Defines learning rate strategy.\n",
    "steps_by_epoch = len(train_data_loader)\n",
    "num_training_steps = steps_by_epoch * num_train_epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_ratio)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型参数保存路径\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step4：提交预测结果\n",
    "\n",
    "加载训练保存的模型加载后进行预测。\n",
    "\n",
    "**NOTE:** 注意设置用于预测的模型参数路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====start training of 0 epochs=====\n",
      "epoch: 0 / 2, steps: 0 / 312, loss: 0.739669, speed: 36.85 step/s\n",
      "epoch: 0 / 2, steps: 50 / 312, loss: 0.219535, speed: 1.13 step/s\n",
      "epoch: 0 / 2, steps: 100 / 312, loss: 0.100981, speed: 1.13 step/s\n",
      "epoch: 0 / 2, steps: 150 / 312, loss: 0.066301, speed: 1.12 step/s\n",
      "epoch: 0 / 2, steps: 200 / 312, loss: 0.048612, speed: 1.11 step/s\n",
      "epoch: 0 / 2, steps: 250 / 312, loss: 0.037864, speed: 1.12 step/s\n",
      "epoch: 0 / 2, steps: 300 / 312, loss: 0.030881, speed: 1.11 step/s\n",
      "epoch time footprint: 0 hour 4 min 38 sec\n",
      "\n",
      "=====start training of 1 epochs=====\n",
      "epoch: 1 / 2, steps: 38 / 312, loss: 0.024896, speed: 1.12 step/s\n",
      "epoch: 1 / 2, steps: 88 / 312, loss: 0.022208, speed: 1.11 step/s\n",
      "epoch: 1 / 2, steps: 138 / 312, loss: 0.019257, speed: 1.12 step/s\n",
      "epoch: 1 / 2, steps: 188 / 312, loss: 0.017365, speed: 1.11 step/s\n",
      "epoch: 1 / 2, steps: 238 / 312, loss: 0.017053, speed: 1.12 step/s\n",
      "epoch: 1 / 2, steps: 288 / 312, loss: 0.014695, speed: 1.12 step/s\n",
      "epoch time footprint: 0 hour 4 min 39 sec\n",
      "\n",
      "=====start evaluating last ckpt of 624 steps=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:07<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.013224\n",
      "precision: 0.00\t recall: 0.00\t f1: 0.00\t\n",
      "\n",
      "=====training complete=====\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# Starts training.\n",
    "global_step = 0\n",
    "logging_steps = 50\n",
    "save_steps = 10000\n",
    "num_train_epochs = 2\n",
    "output_dir = 'checkpoints'\n",
    "tic_train = time.time()\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"\\n=====start training of %d epochs=====\" % epoch)\n",
    "    tic_epoch = time.time()\n",
    "    for step, batch in enumerate(train_data_loader):\n",
    "        input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch\n",
    "        logits = model(input_ids=input_ids)\n",
    "        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and(\n",
    "            (input_ids != 2))\n",
    "        loss = criterion(logits, labels, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_gradients()\n",
    "        loss_item = loss.numpy().item()\n",
    "\n",
    "        if global_step % logging_steps == 0:\n",
    "            print(\n",
    "                \"epoch: %d / %d, steps: %d / %d, loss: %f, speed: %.2f step/s\"\n",
    "                % (epoch, num_train_epochs, step, steps_by_epoch,\n",
    "                    loss_item, logging_steps / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "\n",
    "        if global_step % save_steps == 0 and global_step != 0:\n",
    "            print(\"\\n=====start evaluating ckpt of %d steps=====\" %\n",
    "                    global_step)\n",
    "            precision, recall, f1 = evaluate(\n",
    "                model, criterion, test_data_loader, eval_file_path, \"eval\")\n",
    "            print(\"precision: %.2f\\t recall: %.2f\\t f1: %.2f\\t\" %\n",
    "                    (100 * precision, 100 * recall, 100 * f1))\n",
    "            print(\"saving checkpoing model_%d.pdparams to %s \" %\n",
    "                    (global_step, output_dir))\n",
    "            paddle.save(model.state_dict(),\n",
    "                        os.path.join(output_dir, \n",
    "                                        \"model_%d.pdparams\" % global_step))\n",
    "            model.train()\n",
    "\n",
    "        global_step += 1\n",
    "    tic_epoch = time.time() - tic_epoch\n",
    "    print(\"epoch time footprint: %d hour %d min %d sec\" %\n",
    "            (tic_epoch // 3600, (tic_epoch % 3600) // 60, tic_epoch % 60))\n",
    "\n",
    "# Does final evaluation.\n",
    "print(\"\\n=====start evaluating last ckpt of %d steps=====\" %\n",
    "        global_step)\n",
    "precision, recall, f1 = evaluate(model, criterion, test_data_loader,\n",
    "                                    eval_file_path, \"eval\")\n",
    "print(\"precision: %.2f\\t recall: %.2f\\t f1: %.2f\\t\" %\n",
    "        (100 * precision, 100 * recall, 100 * f1))\n",
    "paddle.save(model.state_dict(),\n",
    "            os.path.join(output_dir,\n",
    "                            \"model_%d.pdparams\" % global_step))\n",
    "print(\"\\n=====training complete=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2021 Baidu.com, Inc. All Rights Reserved\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "import codecs\n",
    "import zipfile\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import DataLoader\n",
    "\n",
    "# from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification, LinearDecayWithWarmup\n",
    "from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer, LinearDecayWithWarmup\n",
    "\n",
    "from data_loader import DuIEDataset, DataCollator\n",
    "from utils import decoding, find_entity, get_precision_recall_f1, write_prediction_results\n",
    "\n",
    "# yapf: disable\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--do_train\", action='store_true', default=False, help=\"do train\")\n",
    "parser.add_argument(\"--do_predict\", action='store_true', default=False, help=\"do predict\")\n",
    "parser.add_argument(\"--init_checkpoint\", default=None, type=str, required=False, help=\"Path to initialize params from\")\n",
    "parser.add_argument(\"--data_path\", default=\"./data\", type=str, required=False, help=\"Path to data.\")\n",
    "parser.add_argument(\"--predict_data_file\", default=\"./data/test_data.json\", type=str, required=False, help=\"Path to data.\")\n",
    "parser.add_argument(\"--output_dir\", default=\"./checkpoints\", type=str, required=False, help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int,help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "    \"than this will be truncated, sequences shorter will be padded.\", )\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\", )\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_ratio\", default=0, type=float, help=\"Linear warmup over warmup_ratio * total_steps.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "args = parser.parse_args()\n",
    "# yapf: enable\n",
    "\n",
    "\n",
    "class BCELossForDuIE(nn.Layer):\n",
    "    def __init__(self, ):\n",
    "        super(BCELossForDuIE, self).__init__()\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels, mask):\n",
    "        loss = self.criterion(logits, labels)\n",
    "        mask = paddle.cast(mask, 'float32')\n",
    "        loss = loss * mask.unsqueeze(-1)\n",
    "        loss = paddle.sum(loss.mean(axis=2), axis=1) / paddle.sum(mask, axis=1)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, data_loader, file_path, mode):\n",
    "    \"\"\"\n",
    "    mode eval:\n",
    "    eval on development set and compute P/R/F1, called between training.\n",
    "    mode predict:\n",
    "    eval on development / test set, then write predictions to \\\n",
    "        predict_test.json and predict_test.json.zip \\\n",
    "        under args.data_path dir for later submission or evaluation.\n",
    "    \"\"\"\n",
    "    example_all = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            example_all.append(json.loads(line))\n",
    "    id2spo_path = os.path.join(os.path.dirname(file_path), \"id2spo.json\")\n",
    "    with open(id2spo_path, 'r', encoding='utf8') as fp:\n",
    "        id2spo = json.load(fp)\n",
    "\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "    eval_steps = 0\n",
    "    formatted_outputs = []\n",
    "    current_idx = 0\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        eval_steps += 1\n",
    "        input_ids, seq_len, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch\n",
    "        logits = model(input_ids=input_ids)\n",
    "        mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and((input_ids != 2))\n",
    "        loss = criterion(logits, labels, mask)\n",
    "        loss_all += loss.numpy().item()\n",
    "        probs = F.sigmoid(logits)\n",
    "        logits_batch = probs.numpy()\n",
    "        seq_len_batch = seq_len.numpy()\n",
    "        tok_to_orig_start_index_batch = tok_to_orig_start_index.numpy()\n",
    "        tok_to_orig_end_index_batch = tok_to_orig_end_index.numpy()\n",
    "        formatted_outputs.extend(decoding(example_all[current_idx: current_idx+len(logits)],\n",
    "                                          id2spo,\n",
    "                                          logits_batch,\n",
    "                                          seq_len_batch,\n",
    "                                          tok_to_orig_start_index_batch,\n",
    "                                          tok_to_orig_end_index_batch))\n",
    "        current_idx = current_idx+len(logits)\n",
    "    loss_avg = loss_all / eval_steps\n",
    "    print(\"eval loss: %f\" % (loss_avg))\n",
    "\n",
    "    if mode == \"predict\":\n",
    "        predict_file_path = os.path.join(args.data_path, 'duie.json')\n",
    "    else:\n",
    "        predict_file_path = os.path.join(args.data_path, 'predict_eval.json')\n",
    "\n",
    "    predict_zipfile_path = write_prediction_results(formatted_outputs,\n",
    "                                                    predict_file_path)\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        precision, recall, f1 = get_precision_recall_f1(file_path,\n",
    "                                                        predict_zipfile_path)\n",
    "        os.system('rm {} {}'.format(predict_file_path, predict_zipfile_path))\n",
    "        return precision, recall, f1\n",
    "    elif mode != \"predict\":\n",
    "        raise Exception(\"wrong mode for eval func\")\n",
    "\n",
    "\n",
    "def do_train():\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    # Reads label_map.\n",
    "    label_map_path = os.path.join(args.data_path, \"predicate2id.json\")\n",
    "    if not (os.path.exists(label_map_path) and os.path.isfile(label_map_path)):\n",
    "        sys.exit(\"{} dose not exists or is not a file.\".format(label_map_path))\n",
    "    with open(label_map_path, 'r', encoding='utf8') as fp:\n",
    "        label_map = json.load(fp)\n",
    "    num_classes = (len(label_map.keys()) - 2) * 2 + 2\n",
    "\n",
    "    # Loads pretrained model ERNIE\n",
    "    # model = ErnieForTokenClassification.from_pretrained(\n",
    "    #     \"ernie-1.0\", num_classes=num_classes)\n",
    "    # model = paddle.DataParallel(model)\n",
    "    # tokenizer = ErnieTokenizer.from_pretrained(\"ernie-1.0\")\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "        \"roberta-wwm-ext-large\",\n",
    "        num_classes=(len(label_map) - 2) * 2 + 2)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")\n",
    "        \n",
    "    criterion = BCELossForDuIE()\n",
    "\n",
    "    # Loads dataset.\n",
    "    train_dataset = DuIEDataset.from_file(\n",
    "        os.path.join(args.data_path, 'train.json'), tokenizer,\n",
    "        args.max_seq_length, True)\n",
    "    train_batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    collator = DataCollator()\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=train_batch_sampler,\n",
    "        collate_fn=collator,\n",
    "        return_list=True)\n",
    "    eval_file_path = os.path.join(args.data_path, 'dev.json')\n",
    "    test_dataset = DuIEDataset.from_file(eval_file_path, tokenizer,\n",
    "                                         args.max_seq_length, True)\n",
    "    test_batch_sampler = paddle.io.BatchSampler(\n",
    "        test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "    test_data_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_sampler=test_batch_sampler,\n",
    "        collate_fn=collator,\n",
    "        return_list=True)\n",
    "\n",
    "    # Defines learning rate strategy.\n",
    "    steps_by_epoch = len(train_data_loader)\n",
    "    num_training_steps = steps_by_epoch * args.num_train_epochs\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_ratio)\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    # Starts training.\n",
    "    global_step = 0\n",
    "    logging_steps = 50\n",
    "    save_steps = 10000\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        print(\"\\n=====start training of %d epochs=====\" % epoch)\n",
    "        tic_epoch = time.time()\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_data_loader):\n",
    "            input_ids, seq_lens, tok_to_orig_start_index, tok_to_orig_end_index, labels = batch\n",
    "            logits = model(input_ids=input_ids)\n",
    "            mask = (input_ids != 0).logical_and((input_ids != 1)).logical_and(\n",
    "                (input_ids != 2))\n",
    "            loss = criterion(logits, labels, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "            loss_item = loss.numpy().item()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % logging_steps == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"epoch: %d / %d, steps: %d / %d, loss: %f, speed: %.2f step/s\"\n",
    "                    % (epoch, args.num_train_epochs, step, steps_by_epoch,\n",
    "                       loss_item, logging_steps / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "\n",
    "            if global_step % save_steps == 0 and rank == 0:\n",
    "                print(\"\\n=====start evaluating ckpt of %d steps=====\" %\n",
    "                      global_step)\n",
    "                precision, recall, f1 = evaluate(\n",
    "                    model, criterion, test_data_loader, eval_file_path, \"eval\")\n",
    "                print(\"precision: %.2f\\t recall: %.2f\\t f1: %.2f\\t\" %\n",
    "                      (100 * precision, 100 * recall, 100 * f1))\n",
    "                print(\"saving checkpoing model_%d.pdparams to %s \" %\n",
    "                      (global_step, args.output_dir))\n",
    "                paddle.save(model.state_dict(),\n",
    "                            os.path.join(args.output_dir,\n",
    "                                         \"model_%d.pdparams\" % global_step))\n",
    "                model.train()  # back to train mode\n",
    "\n",
    "        tic_epoch = time.time() - tic_epoch\n",
    "        print(\"epoch time footprint: %d hour %d min %d sec\" %\n",
    "              (tic_epoch // 3600, (tic_epoch % 3600) // 60, tic_epoch % 60))\n",
    "\n",
    "    # Does final evaluation.\n",
    "    if rank == 0:\n",
    "        print(\"\\n=====start evaluating last ckpt of %d steps=====\" %\n",
    "              global_step)\n",
    "        precision, recall, f1 = evaluate(model, criterion, test_data_loader,\n",
    "                                         eval_file_path, \"eval\")\n",
    "        print(\"precision: %.2f\\t recall: %.2f\\t f1: %.2f\\t\" %\n",
    "              (100 * precision, 100 * recall, 100 * f1))\n",
    "        paddle.save(model.state_dict(),\n",
    "                    os.path.join(args.output_dir,\n",
    "                                 \"model_%d.pdparams\" % global_step))\n",
    "        print(\"\\n=====training complete=====\")\n",
    "\n",
    "\n",
    "def do_predict():\n",
    "    paddle.set_device(args.device)\n",
    "    \n",
    "    # Reads label_map.\n",
    "    label_map_path = os.path.join(args.data_path, \"predicate2id.json\")\n",
    "    if not (os.path.exists(label_map_path) and os.path.isfile(label_map_path)):\n",
    "        sys.exit(\"{} dose not exists or is not a file.\".format(label_map_path))\n",
    "    with open(label_map_path, 'r', encoding='utf8') as fp:\n",
    "        label_map = json.load(fp)\n",
    "    num_classes = (len(label_map.keys()) - 2) * 2 + 2\n",
    "\n",
    "    # Loads pretrained model ERNIE\n",
    "    # model = ErnieForTokenClassification.from_pretrained(\n",
    "    #     \"ernie-1.0\", num_classes=num_classes)\n",
    "    # tokenizer = ErnieTokenizer.from_pretrained(\"ernie-1.0\")\n",
    "    model = RobertaForTokenClassification.from_pretrained(\n",
    "        \"roberta-wwm-ext-large\",\n",
    "        num_classes=(len(label_map) - 2) * 2 + 2)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")\n",
    "\n",
    "    criterion = BCELossForDuIE()\n",
    "\n",
    "    # Loads dataset.\n",
    "    test_dataset = DuIEDataset.from_file(args.predict_data_file, tokenizer,\n",
    "                                         args.max_seq_length, True)\n",
    "    collator = DataCollator()\n",
    "    test_batch_sampler = paddle.io.BatchSampler(\n",
    "        test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "    test_data_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_sampler=test_batch_sampler,\n",
    "        collate_fn=collator,\n",
    "        return_list=True)\n",
    "\n",
    "    # Loads model parameters.\n",
    "    if not (os.path.exists(args.init_checkpoint) and\n",
    "            os.path.isfile(args.init_checkpoint)):\n",
    "        sys.exit(\"wrong directory: init checkpoints {} not exist\".format(\n",
    "            args.init_checkpoint))\n",
    "    state_dict = paddle.load(args.init_checkpoint)\n",
    "    model.set_dict(state_dict)\n",
    "\n",
    "    # Does predictions.\n",
    "    print(\"\\n=====start predicting=====\")\n",
    "    evaluate(model, criterion, test_data_loader, args.predict_data_file,\n",
    "             \"predict\")\n",
    "    print(\"=====predicting complete=====\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if args.do_train:\n",
    "        do_train()\n",
    "    elif args.do_predict:\n",
    "        do_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ export BATCH_SIZE=8\n",
      "+ BATCH_SIZE=8\n",
      "+ export LR=2e-5\n",
      "+ LR=2e-5\n",
      "+ export EPOCH=12\n",
      "+ EPOCH=12\n",
      "+ unset CUDA_VISIBLE_DEVICES\n",
      "+ python -m paddle.distributed.launch --gpus 0 run_duie.py --device gpu --seed 42 --do_train --data_path ./data --max_seq_length 128 --batch_size 8 --num_train_epochs 12 --learning_rate 2e-5 --warmup_ratio 0.06 --output_dir ./checkpoints\n",
      "-----------  Configuration Arguments -----------\n",
      "gpus: 0\n",
      "heter_worker_num: None\n",
      "heter_workers: \n",
      "http_port: None\n",
      "ips: 127.0.0.1\n",
      "log_dir: log\n",
      "nproc_per_node: None\n",
      "run_mode: None\n",
      "server_num: None\n",
      "servers: \n",
      "training_script: run_duie.py\n",
      "training_script_args: ['--device', 'gpu', '--seed', '42', '--do_train', '--data_path', './data', '--max_seq_length', '128', '--batch_size', '8', '--num_train_epochs', '12', '--learning_rate', '2e-5', '--warmup_ratio', '0.06', '--output_dir', './checkpoints']\n",
      "worker_num: None\n",
      "workers: \n",
      "------------------------------------------------\n",
      "WARNING 2021-06-15 11:22:26,648 launch.py:357] Not found distinct arguments and compiled with cuda or xpu. Default use collective mode\n",
      "launch train in GPU mode!\n",
      "INFO 2021-06-15 11:22:26,650 launch_utils.py:510] Local start 1 processes. First process distributed environment info (Only For Debug): \n",
      "    +=======================================================================================+\n",
      "    |                        Distributed Envs                      Value                    |\n",
      "    +---------------------------------------------------------------------------------------+\n",
      "    |                       PADDLE_TRAINER_ID                        0                      |\n",
      "    |                 PADDLE_CURRENT_ENDPOINT                 127.0.0.1:35364               |\n",
      "    |                     PADDLE_TRAINERS_NUM                        1                      |\n",
      "    |                PADDLE_TRAINER_ENDPOINTS                 127.0.0.1:35364               |\n",
      "    |                     PADDLE_RANK_IN_NODE                        0                      |\n",
      "    |                 PADDLE_LOCAL_DEVICE_IDS                        0                      |\n",
      "    |                 PADDLE_WORLD_DEVICE_IDS                        0                      |\n",
      "    |                     FLAGS_selected_gpus                        0                      |\n",
      "    |             FLAGS_selected_accelerators                        0                      |\n",
      "    +=======================================================================================+\n",
      "\n",
      "INFO 2021-06-15 11:22:26,650 launch_utils.py:514] details abouts PADDLE_TRAINER_ENDPOINTS can be found in log/endpoints.log, and detail running logs maybe found in log/workerlog.0\n",
      "launch proc_id:23678 idx:0\n",
      "[2021-06-15 11:22:28,040] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n",
      "W0615 11:22:28.042045 23678 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0615 11:22:28.047034 23678 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "!bash train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ export CUDA_VISIBLE_DEVICES=0\n",
      "+ CUDA_VISIBLE_DEVICES=0\n",
      "+ export BATCH_SIZE=8\n",
      "+ BATCH_SIZE=8\n",
      "+ export CKPT=./checkpoints/model_250000.pdparams\n",
      "+ CKPT=./checkpoints/model_250000.pdparams\n",
      "+ export DATASET_FILE=./data/test.json\n",
      "+ DATASET_FILE=./data/test.json\n",
      "+ python run_duie.py --do_predict --init_checkpoint ./checkpoints/model_250000.pdparams --predict_data_file ./data/test.json --max_seq_length 128 --batch_size 8\n",
      "\u001b[32m[2021-06-16 07:10:14,340] [    INFO]\u001b[0m - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\u001b[0m\n",
      "W0616 07:10:14.342157  3116 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0616 07:10:14.347434  3116 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "!bash predict.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "预测结果会被保存在data/predictions.json，data/predictions.json.zip，其格式与原数据集文件一致。\n",
    "\n",
    "之后可以使用官方评估脚本评估训练模型在dev_data.json上的效果。如：\n",
    "\n",
    "```shell\n",
    "python re_official_evaluation.py --golden_file=dev_data.json  --predict_file=predicitons.json.zip [--alias_file alias_dict]\n",
    "```\n",
    "输出指标为Precision, Recall 和 F1，Alias file包含了合法的实体别名，最终评测的时候会使用，这里不予提供。\n",
    "\n",
    "之后在test_data.json上预测，然后预测结果（.zip文件）至[千言评测页面](https://aistudio.baidu.com/aistudio/competition/detail/46)。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Tricks\n",
    "\n",
    "### 尝试更多的预训练模型\n",
    "\n",
    "基线采用的预训练模型为ERNIE，PaddleNLP提供了丰富的预训练模型，如BERT，RoBERTa，Electra，XLNet等\n",
    "参考[预训练模型文档](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers.html)\n",
    "\n",
    "如可以选择RoBERTa large中文模型优化模型效果，只需更换模型和tokenizer即可无缝衔接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import RobertaForTokenClassification, RobertaTokenizer\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-wwm-ext-large\",\n",
    "    num_classes=(len(label_map) - 2) * 2 + 2)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-wwm-ext-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型集成\n",
    "\n",
    "使用多个模型进行训练预测，将各个模型预测结果进行融合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上基线实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐，及时跟踪最新消息和功能哦**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
